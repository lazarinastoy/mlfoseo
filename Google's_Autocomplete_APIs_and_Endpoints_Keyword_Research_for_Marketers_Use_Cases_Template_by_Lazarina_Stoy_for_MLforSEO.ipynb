{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "cv2hVuqjhJwM",
        "1ScBWiOrC03U"
      ],
      "authorship_tag": "ABX9TyNY8aytAo1X1nayF+ctH0IA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lazarinastoy/mlfoseo/blob/MLinSEO-keyword-research-scripts/Google's_Autocomplete_APIs_and_Endpoints_Keyword_Research_for_Marketers_Use_Cases_Template_by_Lazarina_Stoy_for_MLforSEO.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#About this script"
      ],
      "metadata": {
        "id": "KVIgYKZgiiH2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Copyright (c) 2024 ML Marketing Consulting, ltd. - MLforSEO and Lazarina Stoy.\n",
        "# All rights reserved.\n",
        "\n",
        "# This script is provided as-is, without warranty of any kind, express or implied,\n",
        "# including but not limited to the warranties of merchantability, fitness for a particular purpose,\n",
        "# or noninfringement. In no event shall the authors or copyright holders be liable for any claim,\n",
        "# damages, or other liability, whether in an action of contract, tort, or otherwise, arising from,\n",
        "# out of, or in connection with the software or the use or other dealings in the software.\n",
        "\n",
        "# For any questions, contact: team@mlforseo.com or www.mlforseo.com"
      ],
      "metadata": {
        "id": "F0s-UYmPhXrq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "See the associated tutorial on using this Google Colab on MLforSEO blog - [How to use Google Autocomplete API and Place API for Keyword Suggestions with Python](https://www.mlforseo.com/machine-learning-implementation-guides/keyword-research/how-to-use-google-autocomplete-apis-for-keyword-suggestions/)\n",
        "\n",
        "If you're interested in [ML-enabled Semantic Keyword Research - purchase the course](https://academy.mlforseo.com/product/semantic-ml-enabled-keyword-research-course/)."
      ],
      "metadata": {
        "id": "GahA2Jj-iBOH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Google Search Autocomplete"
      ],
      "metadata": {
        "id": "cv2hVuqjhJwM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "This script is designed to streamline the process of keyword research by leveraging Google Autocomplete. Here's a step-by-step explanation of what it does and how it can be useful:\n",
        "\n",
        "The goal is to generate a comprehensive list of keyword suggestions based on user-provided seed keywords and organize them into clusters. Each suggestion is linked back to its seed keyword for easy analysis.\n",
        "\n",
        "## Key Features:\n",
        "* User-Friendly Input:\n",
        "\n",
        "\n",
        "The user uploads a CSV file containing up to 100 seed keywords.\n",
        "Each keyword serves as a starting point for generating related suggestions.\n",
        "* Automated Keyword Expansion:\n",
        "\n",
        "The script queries Google Suggest for each seed keyword, exploring multiple variations by appending letters (a-z) and blank spaces.\n",
        "This ensures a wide range of suggestions, capturing potential long-tail keywords.\n",
        "* Stopword Filtering:\n",
        "\n",
        "A built-in list of common stopwords removes irrelevant terms, ensuring only meaningful words contribute to clustering.\n",
        "*  Keyword Clustering:\n",
        "\n",
        "Suggestions are grouped based on common terms.\n",
        "Each suggestion is tagged with its corresponding cluster (common word) and the originating seed keyword.\n",
        "* Batch Processing:\n",
        "\n",
        "The script processes keywords in batches of 5 to optimize performance and avoid query throttling by Google.\n",
        "## What you get:\n",
        "\n",
        "The output is a CSV file containing:\n",
        "\n",
        "*  Keyword: The suggested search term.\n",
        "* Cluster: The common word used for grouping.\n",
        "* Seed Keyword: The original keyword that generated the suggestion.\n",
        "\n",
        "\n",
        "This structured format helps users easily identify high-potential keywords and their thematic clusters.\n",
        "\n",
        "## Who Can Benefit?\n",
        "* SEO Professionals: Build content strategies around relevant, high-traffic keywords.\n",
        "* Marketers: Discover trending search terms to improve ad targeting.\n",
        "* Content Creators: Identify long-tail keywords for blog posts, videos, and other media.\n",
        "\n",
        "##How to Use the Script:\n",
        "\n",
        "1. Prepare a CSV file with a column named Keywords containing your seed keywords.\n",
        "2. Upload the file when prompted.\n",
        "3. The script will process the keywords, generate suggestions, and download a clustered keyword file.\n",
        "\n",
        "This tool simplifies the often time-consuming task of keyword suggestion using Google Autosuggest, providing actionable insights in a few simple steps."
      ],
      "metadata": {
        "id": "BUCJUusu7aeT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Step 1: Upload Keyword File\n",
        "\n",
        "import pandas as pd\n",
        "import requests\n",
        "import json\n",
        "import time\n",
        "import string\n",
        "from nltk.tokenize import word_tokenize\n",
        "from collections import Counter\n",
        "from google.colab import files\n",
        "import nltk\n",
        "\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab') # Download the punkt_tab resource\n",
        "\n",
        "# Prompt user to upload a CSV file with keywords\n",
        "uploaded = files.upload()\n",
        "df = pd.read_csv(list(uploaded.keys())[0])  # Read the uploaded file\n",
        "keywords = df['Keywords'].tolist()  # Assume the column is named 'Keywords'\n",
        "\n",
        "## Step 2: Define Helper Functions\n",
        "\n",
        "# Basic stopwords list for English\n",
        "basic_stopwords = {\n",
        "    'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you',\n",
        "    'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself',\n",
        "    'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them',\n",
        "    'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this',\n",
        "    'that', 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been',\n",
        "    'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing',\n",
        "    'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until',\n",
        "    'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between',\n",
        "    'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to',\n",
        "    'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again',\n",
        "    'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why',\n",
        "    'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other',\n",
        "    'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than',\n",
        "    'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now'\n",
        "}\n",
        "\n",
        "def get_google_suggestions(keyword, lang_code, letterlist):\n",
        "    \"\"\"Fetch suggestions for a given keyword and language from Google Suggest.\"\"\"\n",
        "    suggestions = []\n",
        "    headers = {'User-agent': 'Mozilla/5.0'}\n",
        "    for letter in letterlist:\n",
        "        URL = f\"http://suggestqueries.google.com/complete/search?client=firefox&hl={lang_code}&q={keyword} {letter}\"\n",
        "        response = requests.get(URL, headers=headers)\n",
        "        result = json.loads(response.content.decode('utf-8'))\n",
        "        if result:\n",
        "            suggestions.extend(result[1])\n",
        "        time.sleep(0.5)  # Reduced sleep for faster processing\n",
        "    return suggestions\n",
        "\n",
        "def clean_and_cluster_suggestions(all_suggestions, stop_words, seed_words):\n",
        "    \"\"\"Clean suggestions by removing stopwords and tokenize them for clustering.\"\"\"\n",
        "    wordlist = []\n",
        "    for suggestion in all_suggestions:\n",
        "        words = word_tokenize(str(suggestion).lower())\n",
        "        for word in words:\n",
        "            if word not in stop_words and word not in seed_words and len(word) > 1:\n",
        "                wordlist.append(word)\n",
        "    return [word for word, count in Counter(wordlist).most_common(200)]\n",
        "\n",
        "## Step 3: Process Keywords in Batches\n",
        "\n",
        "lang_code = \"en\"  # Language code\n",
        "batch_size = 5\n",
        "letterlist = [\"\"] + list(string.ascii_lowercase)  # Include empty and alphabetical combinations\n",
        "all_clusters = []\n",
        "\n",
        "# Process keywords in batches\n",
        "for i in range(0, len(keywords), batch_size):\n",
        "    batch_keywords = keywords[i:i + batch_size]\n",
        "\n",
        "    # Filter out empty keywords and tokenize seed words\n",
        "    batch_keywords = list(filter(None, batch_keywords))\n",
        "    seed_words = [word_tokenize(keyword.lower()) for keyword in batch_keywords]\n",
        "    seed_words = [item for sublist in seed_words for item in sublist]  # Flatten the list\n",
        "\n",
        "    # Get suggestions for each keyword in the batch\n",
        "    for keyword in batch_keywords:\n",
        "        suggestions = get_google_suggestions(keyword, lang_code, letterlist)\n",
        "        most_common_words = clean_and_cluster_suggestions(suggestions, basic_stopwords, seed_words)\n",
        "\n",
        "        # Assign suggestions and common words to their seed keyword\n",
        "        for common_word in most_common_words:\n",
        "            for suggestion in suggestions:\n",
        "                if common_word in suggestion:\n",
        "                    all_clusters.append([suggestion, common_word, keyword])  # Include the seed keyword here\n",
        "\n",
        "## Step 4: Save and Download the Result\n",
        "\n",
        "cluster_df = pd.DataFrame(all_clusters, columns=['Keyword', 'Cluster', 'Seed Keyword'])\n",
        "cluster_df.to_csv(\"keywords_clustered.csv\", index=False)\n",
        "files.download(\"keywords_clustered.csv\")\n",
        "cluster_df\n"
      ],
      "metadata": {
        "id": "zCmRkdnt6-Zi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36d4cecd-aa46-4abd-baf0-80f791f1823a"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-41ae3031-d892-41f7-af9c-c97e49045297\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-41ae3031-d892-41f7-af9c-c97e49045297\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving seed.csv to seed (1).csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# YouTube Search Autocomplete\n",
        "\n"
      ],
      "metadata": {
        "id": "1ScBWiOrC03U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The process_keywords function, combined with get_youtube_suggestions, is designed to help users extract YouTube search autocomplete suggestions based on a list of seed keywords. These suggestions provide insights into popular search trends and related queries on YouTube.\n",
        "\n",
        "This tool can be especially valuable for content creators, marketers, and researchers who want to identify relevant topics, optimize video content, or analyze user search behavior on YouTube.\n",
        "\n",
        "## Key Features\n",
        "* Bulk Keyword Processing: Upload a CSV file containing multiple keywords to fetch autocomplete suggestions for all of them.\n",
        "* Automated Data Extraction: The function connects to YouTube's suggestion API and retrieves suggestions in real-time.\n",
        "* Customizable Outputs: Generates a downloadable CSV file that pairs each seed keyword with its autocomplete suggestions.\n",
        "* Error Handling: Gracefully handles errors like missing data, connectivity issues, or API response anomalies.\n",
        "* Rate-Limiting Compliance: Includes delays between API calls to avoid hitting request limits.\n",
        "\n",
        "\n",
        "## Who Can Benefit\n",
        "* YouTube Content Creators: Discover trending topics and long-tail keywords to improve video SEO and reach a broader audience.\n",
        "* Digital Marketers: Identify search terms to optimize ad campaigns or create targeted content strategies.\n",
        "* SEO Professionals: Research user behavior and improve keyword targeting for better ranking on search engines.\n",
        "* Academics & Researchers: Analyze search trends and user interests on YouTube for academic studies or reports.\n",
        "* Businesses: Gain insights into customer preferences to tailor product or service-related content on YouTube.\n",
        "\n",
        "## How to Use\n",
        "1. Prepare Your Keywords File:\n",
        "\n",
        "* Create a CSV file with a single column named Keywords.\n",
        "*  Populate this column with the seed keywords you want to analyze.\n",
        "2. Upload the File:\n",
        "\n",
        "* Run the notebook cell containing files.upload() to upload your CSV file.\n",
        "3. Run the Analysis:\n",
        "\n",
        "* The script will process the uploaded file, extract autocomplete suggestions for each keyword, and store the results in a DataFrame.\n",
        "4. Download the Results:\n",
        "\n",
        "*  The suggestions are saved in a CSV file named youtube_autosuggestions.csv, which can be downloaded for further use.\n",
        "5. Explore the Data:\n",
        "\n",
        "* Review the suggestions to identify trends, optimize your content, or use them in your marketing strategies.\n"
      ],
      "metadata": {
        "id": "fDPrEwL1hExj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import requests\n",
        "import pandas as pd\n",
        "import json\n",
        "import time\n",
        "from google.colab import files\n",
        "\n",
        "def get_youtube_suggestions(keyword):\n",
        "    \"\"\"\n",
        "    Fetch YouTube autocomplete suggestions for a given keyword.\n",
        "\n",
        "    Parameters:\n",
        "    keyword (str): The search query string.\n",
        "\n",
        "    Returns:\n",
        "    list of tuples: Each tuple contains the seed keyword and its suggestion.\n",
        "    \"\"\"\n",
        "    suggestions = []\n",
        "    try:\n",
        "        url = \"https://suggestqueries.google.com/complete/search\"\n",
        "        params = {\n",
        "            'client': 'youtube',\n",
        "            'ds': 'yt',\n",
        "            'q': keyword,\n",
        "            'hl': 'en'\n",
        "        }\n",
        "        response = requests.get(url, params=params)\n",
        "        response.raise_for_status()\n",
        "        raw_data = response.text\n",
        "\n",
        "        # Extract JSON-like content from the JavaScript response\n",
        "        start = raw_data.find('[')\n",
        "        end = raw_data.rfind(']') + 1\n",
        "        json_data = json.loads(raw_data[start:end])\n",
        "\n",
        "        # Process suggestions\n",
        "        seed_keyword = json_data[0]\n",
        "        for item in json_data[1]:\n",
        "            suggestions.append((seed_keyword, item[0]))\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching suggestions for '{keyword}': {e}\")\n",
        "\n",
        "    return suggestions\n",
        "\n",
        "def process_keywords(file_path):\n",
        "    \"\"\"\n",
        "    Process a list of keywords from an uploaded file and fetch YouTube suggestions.\n",
        "\n",
        "    Parameters:\n",
        "    file_path (str): Path to the uploaded CSV file containing a 'Keywords' column.\n",
        "\n",
        "    Returns:\n",
        "    DataFrame: A DataFrame containing the seed keywords and their suggestions.\n",
        "    \"\"\"\n",
        "    df = pd.read_csv(file_path)\n",
        "    print(\"Uploaded file columns:\", df.columns)  # Debug: Print column names\n",
        "\n",
        "    if 'Keywords' not in df.columns:\n",
        "        raise ValueError(\"The uploaded file must contain a 'Keywords' column.\")\n",
        "\n",
        "    all_suggestions = []\n",
        "\n",
        "    for keyword in df['Keywords'].dropna():\n",
        "        suggestions = get_youtube_suggestions(keyword)\n",
        "        all_suggestions.extend(suggestions)\n",
        "        time.sleep(0.5)  # To prevent hitting rate limits\n",
        "\n",
        "    result_df = pd.DataFrame(all_suggestions, columns=['Seed Keyword', 'Suggestion'])\n",
        "    return result_df\n",
        "\n",
        "# Step 1: Upload the keywords file\n",
        "uploaded = files.upload()\n",
        "file_path = next(iter(uploaded.keys()))\n",
        "\n",
        "# Step 2: Process the keywords and fetch suggestions\n",
        "try:\n",
        "    suggestions_df = process_keywords(file_path)\n",
        "\n",
        "    # Step 3: Save and download the results\n",
        "    output_file = \"youtube_autosuggestions.csv\"\n",
        "    suggestions_df.to_csv(output_file, index=False)\n",
        "    files.download(output_file)\n",
        "\n",
        "    # Display first few rows of the DataFrame\n",
        "    suggestions_df.head()\n",
        "except Exception as e:\n",
        "    print(f\"Error: {e}\")\n"
      ],
      "metadata": {
        "id": "qG_6BCywC0k_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Query Autocomplete via Place API"
      ],
      "metadata": {
        "id": "6iFImej3gwnq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Google’s Query Autocomplete service helps users find location-based suggestions as they type. By using the Places API, you can implement autocomplete suggestions for geographical searches, such as searching for \"pizza near New York\" and getting relevant suggestions like \"pizza near Paris\" or \"pizza near Disneyland.\"\n",
        "\n",
        "## Key Features:\n",
        "Provides query predictions for geographical searches.\n",
        "Returns suggestions in real-time based on partial input.\n",
        "Can include geographic details, like addresses and locations.\n",
        "Supports language customization for better local results.\n",
        "## How It Works:\n",
        "A Query Autocomplete request is made via a URL, where users provide an input (search term) and, optionally, other parameters like language, location, and radius. The API returns location-based suggestions based on user input.\n",
        "\n",
        "You can specify additional parameters, such as:\n",
        "\n",
        "* language: Set the desired language for results.\n",
        "* location: Bias results to a certain location.\n",
        "* radius: Define the distance within which to return results.\n",
        "\n",
        "## Key Considerations:\n",
        "\n",
        "* API Key: You must enable the API in a Google Cloud project with enabled billing beforehand to get your API key.\n",
        "* Quota Limits: The Query Autocomplete API shares quotas with other Google Maps services. Make sure you're within the API limits.\n",
        "* Response Structure: The API response includes a list of predictions, each containing a human-readable description of the suggestion, matched substrings, and structured formatting for easy display.\n",
        "\n",
        "## Example Response:\n",
        "For an input like pizza near par, you might receive suggestions like:\n",
        "\n",
        "* pizza near Paris, France\n",
        "* pizza near Paris Beauvais Airport, France\n",
        "* pizza near Disneyland Park, California, USA\n",
        "\n",
        "## Why It’s Useful:\n",
        "* Search Suggestions: Ideal for applications where users need to type locations or businesses. It helps them find places based on their input.\n",
        "* Local Optimization: Helps users find locations near them or within a defined area.\n",
        "* Global Reach: Supports multiple languages and location-based filtering, making it ideal for international use.\n",
        "\n",
        "##Key Features:\n",
        "* User Input: The user is prompted to enter their Google API key, which is required for the Places API. Additionally, the user can specify the language code, location (latitude and longitude), and search radius (in meters) for the query.\n",
        "\n",
        "* File Upload: The user is able to upload a CSV file containing a column named \"Keywords,\" which will be used to fetch autocomplete suggestions from the Google Places API.\n",
        "\n",
        "* Query Autocomplete: The script fetches autocomplete suggestions from the Google Places API for each keyword in the \"Keywords\" column of the uploaded CSV file. This is done by sending a request to the API for each keyword.\n",
        "\n",
        "* Downloadable Output: After fetching the suggestions, the results are stored in a new CSV file. The user can then download this file, which contains the original seed keywords along with their corresponding suggestions.\n",
        "##Notes:\n",
        "* The user must replace YOUR_API_KEY with their actual Google API key.\n",
        "* The code expects the uploaded CSV file to have a \"Keywords\" column containing search terms. You can also specify other optional parameters like location and radius to bias the suggestions based on location.\n",
        "* A delay of 0.5 seconds between requests is included to avoid hitting the API rate limits.\n",
        "\n"
      ],
      "metadata": {
        "id": "o8nEOzFpiutt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "import json\n",
        "import time\n",
        "from google.colab import files\n",
        "\n",
        "def get_place_autocomplete_suggestions(input_keyword, api_key, language='en', location=None, radius=50000):\n",
        "    \"\"\"\n",
        "    Fetch Google Places Query Autocomplete suggestions for a given input keyword.\n",
        "\n",
        "    Parameters:\n",
        "    input_keyword (str): The input text string for autocomplete.\n",
        "    api_key (str): The Google API key.\n",
        "    language (str): The language for the query results (default is 'en').\n",
        "    location (tuple): Latitude and longitude to bias the search (optional).\n",
        "    radius (int): Search radius in meters (default is 50,000 meters).\n",
        "\n",
        "    Returns:\n",
        "    list of tuples: Each tuple contains the input keyword and a predicted place description.\n",
        "    \"\"\"\n",
        "    suggestions = []\n",
        "\n",
        "    try:\n",
        "        url = \"https://maps.googleapis.com/maps/api/place/queryautocomplete/json\"\n",
        "        params = {\n",
        "            'input': input_keyword,\n",
        "            'key': api_key,\n",
        "            'language': language,\n",
        "            'radius': radius\n",
        "        }\n",
        "\n",
        "        # Add location bias if provided\n",
        "        if location:\n",
        "            params['location'] = f\"{location[0]},{location[1]}\"\n",
        "\n",
        "        response = requests.get(url, params=params)\n",
        "        response.raise_for_status()\n",
        "        data = response.json()\n",
        "\n",
        "        # Process the suggestions from the response\n",
        "        if data.get('status') == 'OK':\n",
        "            for prediction in data.get('predictions', []):\n",
        "                suggestions.append((input_keyword, prediction['description']))\n",
        "        else:\n",
        "            print(f\"Error fetching suggestions for '{input_keyword}': {data.get('error_message', 'No suggestions found')}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "\n",
        "    return suggestions\n",
        "\n",
        "def process_keywords(file_path, api_key, language='en', location=None, radius=50000):\n",
        "    \"\"\"\n",
        "    Process a list of keywords from an uploaded file and fetch Google Places query suggestions.\n",
        "\n",
        "    Parameters:\n",
        "    file_path (str): Path to the uploaded CSV file containing a 'Keywords' column.\n",
        "    api_key (str): The Google API key.\n",
        "    language (str): The language for the query results (default is 'en').\n",
        "    location (tuple): Latitude and longitude to bias the search (optional).\n",
        "    radius (int): Search radius in meters (default is 50,000 meters).\n",
        "\n",
        "    Returns:\n",
        "    DataFrame: A DataFrame containing the seed keywords and their suggestions.\n",
        "    \"\"\"\n",
        "    df = pd.read_csv(file_path)\n",
        "    print(\"Uploaded file columns:\", df.columns)  # Debug: Print column names\n",
        "\n",
        "    if 'Keywords' not in df.columns:\n",
        "        raise ValueError(\"The uploaded file must contain a 'Keywords' column.\")\n",
        "\n",
        "    all_suggestions = []\n",
        "\n",
        "    # Fetch suggestions for each keyword in the 'Keywords' column\n",
        "    for keyword in df['Keywords'].dropna():\n",
        "        suggestions = get_place_autocomplete_suggestions(keyword, api_key, language, location, radius)\n",
        "        all_suggestions.extend(suggestions)\n",
        "        time.sleep(0.5)  # To prevent hitting rate limits\n",
        "\n",
        "    result_df = pd.DataFrame(all_suggestions, columns=['Seed Keyword', 'Suggestion'])\n",
        "    return result_df\n",
        "\n",
        "# Step 1: Request user input for API key and parameters\n",
        "api_key = input(\"Please enter your Google API key: \")\n",
        "language = input(\"Enter language code (default is 'en'): \") or 'en'\n",
        "location_input = input(\"Enter location (latitude,longitude) or press Enter to skip: \")\n",
        "location = tuple(map(float, location_input.split(','))) if location_input else None\n",
        "radius = int(input(\"Enter search radius in meters (default is 50000): \") or 50000)\n",
        "\n",
        "# Step 2: Upload the keywords file\n",
        "uploaded = files.upload()\n",
        "file_path = next(iter(uploaded.keys()))\n",
        "\n",
        "# Step 3: Process the keywords and fetch suggestions\n",
        "try:\n",
        "    suggestions_df = process_keywords(file_path, api_key, language, location, radius)\n",
        "\n",
        "    # Step 4: Save and download the results\n",
        "    output_file = \"place_autosuggestions.csv\"\n",
        "    suggestions_df.to_csv(output_file, index=False)\n",
        "    files.download(output_file)\n",
        "\n",
        "    # Display first few rows of the DataFrame\n",
        "    suggestions_df.head()\n",
        "except Exception as e:\n",
        "    print(f\"Error: {e}\")\n"
      ],
      "metadata": {
        "id": "vnsRJuf_iub-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Place Autocomplete via Place API"
      ],
      "metadata": {
        "id": "68f7DIKlq9SB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "import json\n",
        "import time\n",
        "from google.colab import files\n",
        "\n",
        "def get_place_autocomplete_suggestions(input_text, api_key, **kwargs):\n",
        "    \"\"\"\n",
        "    Fetch Place Autocomplete suggestions for a given input text.\n",
        "\n",
        "    Parameters:\n",
        "    input_text (str): The input string for place autocomplete.\n",
        "    api_key (str): Your Google Places API key.\n",
        "    kwargs (dict): Optional parameters for the Place Autocomplete API such as:\n",
        "                   - location\n",
        "                   - radius\n",
        "                   - language\n",
        "                   - components\n",
        "                   - types\n",
        "\n",
        "    Returns:\n",
        "    list of tuples: Each tuple contains the input text and the suggested place description.\n",
        "    \"\"\"\n",
        "    suggestions = []\n",
        "    try:\n",
        "        url = \"https://maps.googleapis.com/maps/api/place/autocomplete/json\"\n",
        "        params = {\n",
        "            'input': input_text,\n",
        "            'key': api_key,\n",
        "            **kwargs\n",
        "        }\n",
        "\n",
        "        response = requests.get(url, params=params)\n",
        "        response.raise_for_status()\n",
        "\n",
        "        data = response.json()\n",
        "\n",
        "        if data.get(\"status\") == \"OK\":\n",
        "            for prediction in data.get(\"predictions\", []):\n",
        "                suggestions.append((input_text, prediction.get(\"description\")))\n",
        "        else:\n",
        "            print(f\"Error: {data.get('status')} - {data.get('error_message', 'No error message')}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching suggestions for '{input_text}': {e}\")\n",
        "\n",
        "    return suggestions\n",
        "\n",
        "def process_autocomplete_keywords(file_path, api_key, **kwargs):\n",
        "    \"\"\"\n",
        "    Process a list of input texts from an uploaded file and fetch Place Autocomplete suggestions.\n",
        "\n",
        "    Parameters:\n",
        "    file_path (str): Path to the uploaded CSV file containing a 'Keywords' column.\n",
        "    api_key (str): Your Google Places API key.\n",
        "    kwargs (dict): Optional parameters for Place Autocomplete API.\n",
        "\n",
        "    Returns:\n",
        "    DataFrame: A DataFrame containing the input texts and their suggestions.\n",
        "    \"\"\"\n",
        "    df = pd.read_csv(file_path)\n",
        "    print(\"Uploaded file columns:\", df.columns)  # Debug: Print column names\n",
        "\n",
        "    if 'Keywords' not in df.columns:\n",
        "        raise ValueError(\"The uploaded file must contain a 'Keywords' column.\")\n",
        "\n",
        "    all_suggestions = []\n",
        "\n",
        "    for keyword in df['Keywords'].dropna():\n",
        "        suggestions = get_place_autocomplete_suggestions(keyword, api_key, **kwargs)\n",
        "        all_suggestions.extend(suggestions)\n",
        "        time.sleep(0.5)  # To prevent hitting rate limits\n",
        "\n",
        "    result_df = pd.DataFrame(all_suggestions, columns=['Input Text', 'Suggestion'])\n",
        "    return result_df\n",
        "\n",
        "# Step 1: Upload the keywords file\n",
        "uploaded = files.upload()\n",
        "file_path = next(iter(uploaded.keys()))\n",
        "\n",
        "# Step 2: Get API key and optional parameters from the user\n",
        "api_key = input(\"Enter your Google Places API key: \")\n",
        "location = input(\"Enter location as 'latitude,longitude' (or leave blank): \").strip()\n",
        "radius = input(\"Enter radius in meters (or leave blank): \").strip()\n",
        "language = input(\"Enter language code (e.g., 'en', 'fr') (or leave blank): \").strip()\n",
        "components = input(\"Enter components (e.g., 'country:us') (or leave blank): \").strip()\n",
        "types = input(\"Enter types (e.g., 'geocode') (or leave blank): \").strip()\n",
        "\n",
        "# Collect optional parameters\n",
        "optional_params = {k: v for k, v in {\n",
        "    'location': location,\n",
        "    'radius': radius,\n",
        "    'language': language,\n",
        "    'components': components,\n",
        "    'types': types\n",
        "}.items() if v}\n",
        "\n",
        "# Step 3: Process the keywords and fetch autocomplete suggestions\n",
        "try:\n",
        "    suggestions_df = process_autocomplete_keywords(file_path, api_key, **optional_params)\n",
        "\n",
        "    # Step 4: Save and download the results\n",
        "    output_file = \"place_autocomplete_suggestions.csv\"\n",
        "    suggestions_df.to_csv(output_file, index=False)\n",
        "    files.download(output_file)\n",
        "\n",
        "    # Display first few rows of the DataFrame\n",
        "    suggestions_df.head()\n",
        "except Exception as e:\n",
        "    print(f\"Error: {e}\")\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "dRqd9558rBip"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}